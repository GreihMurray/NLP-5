{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Seq2Seq Machine Translation"
      ],
      "metadata": {
        "id": "7Yp1O0SaVnLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import re\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s5tnu3eJJRXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03def221-d436-4851-a63d-49652afc9c8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to convert source tokens into lists of complete sentences  "
      ],
      "metadata": {
        "id": "gzsdiPwXblbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_source_sentences(data):\n",
        "  sentence = []\n",
        "  sentences=[]\n",
        "  for line in data:\n",
        "    token = line.rstrip(\"\\n\")\n",
        "    if token == '<s>':\n",
        "      sentence = []\n",
        "      sen=''\n",
        "    elif token=='</s>':\n",
        "      sen+=' '.join(sentence)\n",
        "      sentences.append(sen)\n",
        "    else:\n",
        "      sentence.append(token)\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "4z3Cac0_0eDo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to convert target tokens into list of complete sentences "
      ],
      "metadata": {
        "id": "VheyAJyObyFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_target_sentences(data):\n",
        "  sentence = []\n",
        "  sentences=[]\n",
        "  for line in data:\n",
        "    token = line.rstrip(\"\\n\")\n",
        "    if token == '<s>':\n",
        "      sentence = []\n",
        "      sen='\\t'\n",
        "    elif token=='</s>':\n",
        "      sen+=' '.join(sentence)\n",
        "      sen+='\\n'\n",
        "      sentences.append(sen)\n",
        "    else:\n",
        "      sentence.append(token)\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "hrvpkGGeUWKy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of Source/Target Characters Function"
      ],
      "metadata": {
        "id": "rh93MirAjvVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inp_out_characters(source_sentences,target_sentences):\n",
        "  input_characters = set()\n",
        "  target_characters = set()\n",
        "  for sentence in source_sentences:\n",
        "      for char in sentence:\n",
        "          if char not in input_characters:\n",
        "              input_characters.add(char)\n",
        "\n",
        "  for sentence in target_sentences:\n",
        "      for char in sentence:\n",
        "          if char not in target_characters:\n",
        "              target_characters.add(char)\n",
        "  input_characters = sorted(list(input_characters))\n",
        "  target_characters = sorted(list(target_characters))\n",
        "  return input_characters,target_characters"
      ],
      "metadata": {
        "id": "vAbznlRHVmH3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Stats Function"
      ],
      "metadata": {
        "id": "RUKx-jfjj1b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_stats(input_characters,target_characters,source_sentences,target_sentences): \n",
        "  num_encoder_tokens = len(input_characters) \n",
        "  num_decoder_tokens = len(target_characters)\n",
        "  max_encoder_seq_length = max([len(txt) for txt in source_sentences])\n",
        "  max_decoder_seq_length = max([len(txt) for txt in target_sentences])\n",
        "  return num_encoder_tokens,num_decoder_tokens,max_encoder_seq_length,max_decoder_seq_length\n"
      ],
      "metadata": {
        "id": "qmpsxfA5XacG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Data"
      ],
      "metadata": {
        "id": "8AjqTAoSkZ3_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hXYqSWZPz9Xf"
      },
      "outputs": [],
      "source": [
        "train_source=open('/content/drive/MyDrive/Colab Notebooks/train-source.txt','r',encoding = \"UTF-8\").readlines()\n",
        "test_source=open('/content/drive/MyDrive/Colab Notebooks/train-target.txt','r',encoding = \"UTF-8\").readlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_sentences=conv_source_sentences(train_source)\n",
        "target_sentences=conv_target_sentences(test_source)\n",
        "print(\"Source Sentences: \\n\",source_sentences[:1])\n",
        "print(\"Target Sentences: \\n\",target_sentences[:1])"
      ],
      "metadata": {
        "id": "e2AFWJhay7M3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97ddf3d-160c-4101-f2ec-3363e4dfa80b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentences: \n",
            " ['Cinnte go leór , thiocfadh dóbhtha bás a fhagháil ar imeall an phuill udaí .']\n",
            "Target Sentences: \n",
            " ['\\tCinnte go leor , thiocfadh dóibh bás a fháil ar imeall an phoill úd .\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_characters,target_characters=inp_out_characters(source_sentences,target_sentences)\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
      ],
      "metadata": {
        "id": "G5TSZk0IaMoF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens,num_decoder_tokens,max_encoder_seq_length,max_decoder_seq_length=data_stats(input_characters,target_characters,source_sentences,target_sentences)\n",
        "print(\"Number of samples:\", len(source_sentences))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqJ2iMHwUy_3",
        "outputId": "0fb2bdff-e11f-4ac2-835e-33b0998f9661"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 45171\n",
            "Number of unique input tokens: 107\n",
            "Number of unique output tokens: 96\n",
            "Max sequence length for inputs: 1190\n",
            "Max sequence length for outputs: 1115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Validation Split"
      ],
      "metadata": {
        "id": "XZkdY5vJkjJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(source_sentences, target_sentences, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "iyCa2ICgF8rF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 128) :\n",
        "  #' Generate a batch of data \n",
        "  while True:\n",
        "    for j in range(0, len(X), batch_size):\n",
        "      encoder_input_data = np.zeros((batch_size, max_encoder_seq_length), dtype='float32')\n",
        "      decoder_input_data = np.zeros((batch_size, max_decoder_seq_length) , dtype= 'float32')\n",
        "      decoder_target_data = np.zeros((batch_size, max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "      for i,(input_text, target_text) in enumerate(zip (X[j:j+batch_size], y[j:j+batch_size])):\n",
        "        #print(input_text)\n",
        "        for t, word in enumerate(input_text):\n",
        "          #print(word)\n",
        "          encoder_input_data[i, t] = input_token_index[word] # encoder input seg\n",
        "        for t, word in enumerate(target_text):\n",
        "          if t<len(target_text)-1:\n",
        "            decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
        "          if t>0:\n",
        "            # decoder target sequence (one hot encoded)\n",
        "            # does not include the sTART token\n",
        "            # Offset by one timestep\n",
        "            decoder_target_data[i, t - 1, target_token_index [word]] = 1.\n",
        "      yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "metadata": {
        "id": "Vrb8DdchXaiX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Architecture"
      ],
      "metadata": {
        "id": "vhXr9W0Zxckz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input sequence and process it.\n",
        "latent_dim=256\n",
        "encoder_inputs = keras.Input(shape=(None,))\n",
        "enc_emb=Embedding(num_encoder_tokens,latent_dim,mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n"
      ],
      "metadata": {
        "id": "kedzpB7yGnVy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder Architecture"
      ],
      "metadata": {
        "id": "pEdsiloO6o9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None,))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "dec_emb_layer=Embedding(num_decoder_tokens,latent_dim,mask_zero=True)\n",
        "dec_emb=dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "eic5Y57R6qt_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 400\n",
        "epochs = 20\n",
        "#print(val_samples//batch_size)"
      ],
      "metadata": {
        "id": "foFoWZHQ6-0h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator function to iteratively pull data and fit the model"
      ],
      "metadata": {
        "id": "SQegxhCRkq08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "metadata": {
        "id": "fH3pae6o6pK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('/content/drive/MyDrive/Colab Notebooks/machine_translation_model.h5')"
      ],
      "metadata": {
        "id": "A8bI_GOUs5k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/drive/MyDrive/Colab Notebooks/machine_translation_model.h5')"
      ],
      "metadata": {
        "id": "tC4vhhOHNbjy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "#decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "dec_emb2=dec_emb_layer(decoder_inputs)\n",
        "#decoder_lstm = model.layers[3]\n",
        "\n",
        "decoder_outputs2, state_h_dec, state_c_dec = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h_dec, state_c_dec]\n",
        "#decoder_dense = model.layers[4]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "zKOVGiWYbIh9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "metadata": {
        "id": "ffeuTSYObZ8K"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode Sequence Function"
      ],
      "metadata": {
        "id": "IwCr_wNTkzsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq,verbose=0)\n",
        "    #print(\"state value predictions: \",states_value)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0]= target_token_index['\\t']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value,verbose=0)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "            #print(\"Inside break cond\")\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] =sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "GTCHi8z6HBtv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen=generate_batch(X_train,y_train,batch_size=1)\n",
        "k=-1"
      ],
      "metadata": {
        "id": "0cFpSdCcjULp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XFhpG-RG-Krr",
        "outputId": "23f8cde8-93c8-40b5-b6dc-a43177c1727f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cé go rabh siad ag bogadaigh go scaolmhar níor theich siad go rabh mé fá leath-duisín slat dóbhtha .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K0TLQW2w-mU9",
        "outputId": "59d1b18b-c2b5-481d-e942-f42282c96bfe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\tCé go raibh siad ag bogadaigh go scaollmhar níor theith siad go raibh mé fá leathdhoisín slat dóibh .\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k+=1\n",
        "print(k)\n",
        "(input_seq,actual_output),_=next(train_gen)\n",
        "decoded_sentence=decode_sequence(input_seq)\n",
        "print(input_seq[0].shape)\n",
        "print('Input Source sentence:', X_train[k:k+1])\n",
        "print('Actual Target Translation:', y_train[k:k+1])\n",
        "print('Predicted Target Translation:', decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoaNmsr1jjWa",
        "outputId": "ed9b7589-ac9f-4c4e-f6b1-f46d5da5c308"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "(1190,)\n",
            "Input Source sentence: [\"Lá ar n-a bhárach , bhí tamall eile comhráidh eadar an saighdiuir céadna a's an Gasúr Mór .\"]\n",
            "Actual Target Translation: ['\\tLá arna mhárach bhí tamall eile comhrá idir an saighdiúir céanna is an Gasúr Mór .\\n']\n",
            "Predicted Target Translation: I dtabhairt an t-am sin agus an t-am sin agus an t-am sin agus an t-am sin a bhí ann .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions & Evaluation"
      ],
      "metadata": {
        "id": "Iyy97j6UQx38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def anything_goes_translation(src_sentence,max_encoder_seq_length):\n",
        "  encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
        "  for i,input_text in enumerate(src_sentence):\n",
        "    for t, word in enumerate(input_text):\n",
        "      encoder_input_data[i, t] = input_token_index[word] # encoder input seg\n",
        "  decoded_sentence=decode_sequence(encoder_input_data)\n",
        "  return decoded_sentence\n"
      ],
      "metadata": {
        "id": "r5E5KWJw7-zL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "6zDQKC5UqJU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(max_encoder_seq_length):\n",
        "  testsource = open('test-source.txt', 'r')\n",
        "  anything_goes_hypotheses = []\n",
        "  count=0\n",
        "  for line in testsource:\n",
        "    token = line.rstrip(\"\\n\")\n",
        "    if token == '<s>':\n",
        "      sentence = []\n",
        "      sen=''\n",
        "    elif token == '</s>':\n",
        "      sentences = []\n",
        "      sen+=' '.join(sentence)\n",
        "      sentences.append(sen)\n",
        "      anything_goes_hypotheses.append(anything_goes_translation(sentences,max_encoder_seq_length))\n",
        "    else:\n",
        "      sentence.append(token)\n",
        "  references = []\n",
        "  testtarget = open('test-target.txt', 'r')\n",
        "  for line in testtarget:\n",
        "    token = line.rstrip(\"\\n\")\n",
        "    if token == '<s>':\n",
        "      sentence = []\n",
        "      sen=''\n",
        "    elif token == '</s>':\n",
        "      sen+=' '.join(sentence)\n",
        "      references.append([sen])\n",
        "    else:\n",
        "      sentence.append(token)\n",
        "  return corpus_bleu(references,anything_goes_hypotheses)"
      ],
      "metadata": {
        "id": "wZXv09LROT28"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score=evaluate(max_encoder_seq_length)"
      ],
      "metadata": {
        "id": "NPXhnEE4nb9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bleu Score: \",score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3C_WELseTW3",
        "outputId": "6eced5fe-2809-4dfe-f9d8-269d880eaa55"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bleu Score:  0.18612349262388178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have saved returned list of all decoded sequences"
      ],
      "metadata": {
        "id": "LcJXTz_Rmcfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/anything_goes_preds_updated', 'wb') as file:\n",
        "  pickle.dump(anything_goes_hypotheses, file)"
      ],
      "metadata": {
        "id": "8ZPS5unV0HsO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/anything_goes_preds_updated','rb') as file:\n",
        "    anything_goes_hypotheses = pickle.load(file)"
      ],
      "metadata": {
        "id": "IqUnQbHaDJNK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of decoded sequences list \",len(anything_goes_hypotheses))\n",
        "print(\"Length of references sequences list \",len(references))\n",
        "print(\"Decoded sequences list \",anything_goes_hypotheses[:2])\n",
        "print(\"References sequences list \",references[:2])"
      ],
      "metadata": {
        "id": "MwZevBR2Wu8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdef0102-3bd2-4c9c-8d7d-a1eebae77e42"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of decoded sequences list  1000\n",
            "Length of references sequences list  1000\n",
            "Decoded sequences list  ['Is é an chéad chuid an tseanbhean agus an t-am sin agus an t-am sin .\\n', 'I mo cheann a bhí ann , agus an t-am sin agus an t-am sin agus an t-am .\\n']\n",
            "References sequences list  [['Scéal Chathail Freeman - Téann mo dheartháir chun na Dúcharraige'], ['Mí Iúil a bhí ann i mbliain a 1854 , nuair a bhain an taisme seo dúinn .']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score=corpus_bleu(references,anything_goes_hypotheses)\n",
        "print(\"Bleu Score: \",score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkhnfmMneA1p",
        "outputId": "10b494fd-728b-4889-836c-7b00bc65cdf9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bleu Score:  0.18612349262388178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IodiGNhPeUcb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}