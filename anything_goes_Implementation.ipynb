{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Seq2Seq Machine Translation"
      ],
      "metadata": {
        "id": "7Yp1O0SaVnLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s5tnu3eJJRXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e7ee3cc-0d37-4b76-f2a8-62ccaacc8761"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to convert source tokens into lists of complete sentences  "
      ],
      "metadata": {
        "id": "gzsdiPwXblbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_source_sentences(data):\n",
        "  sentence = []\n",
        "  sentences=[]\n",
        "  for line in data:\n",
        "    token = line.rstrip(\"\\n\")\n",
        "    if token == '<s>':\n",
        "      sentence = []\n",
        "      sen=''\n",
        "    elif token=='</s>':\n",
        "      sen+=' '.join(sentence)\n",
        "      sentences.append(sen)\n",
        "    else:\n",
        "      sentence.append(token)\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "4z3Cac0_0eDo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to convert target tokens into list of complete sentences "
      ],
      "metadata": {
        "id": "VheyAJyObyFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_target_sentences(data):\n",
        "  sentence = []\n",
        "  sentences=[]\n",
        "  for line in data:\n",
        "    token = line.rstrip(\"\\n\")\n",
        "    if token == '<s>':\n",
        "      sentence = []\n",
        "      sen='\\t'\n",
        "    elif token=='</s>':\n",
        "      sen+=' '.join(sentence)\n",
        "      sen+='\\n'\n",
        "      sentences.append(sen)\n",
        "    else:\n",
        "      sentence.append(token)\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "hrvpkGGeUWKy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of Source/Target Characters Function"
      ],
      "metadata": {
        "id": "rh93MirAjvVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inp_out_characters(source_sentences,target_sentences):\n",
        "  input_characters = set()\n",
        "  target_characters = set()\n",
        "  for sentence in source_sentences:\n",
        "      for char in sentence:\n",
        "          if char not in input_characters:\n",
        "              input_characters.add(char)\n",
        "\n",
        "  for sentence in target_sentences:\n",
        "      for char in sentence:\n",
        "          if char not in target_characters:\n",
        "              target_characters.add(char)\n",
        "  input_characters = sorted(list(input_characters))\n",
        "  target_characters = sorted(list(target_characters))\n",
        "  return input_characters,target_characters"
      ],
      "metadata": {
        "id": "vAbznlRHVmH3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Stats Function"
      ],
      "metadata": {
        "id": "RUKx-jfjj1b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_stats(input_characters,target_characters,source_sentences,target_sentences): \n",
        "  num_encoder_tokens = len(input_characters) \n",
        "  num_decoder_tokens = len(target_characters)\n",
        "  max_encoder_seq_length = max([len(txt) for txt in source_sentences])\n",
        "  max_decoder_seq_length = max([len(txt) for txt in target_sentences])\n",
        "  return num_encoder_tokens,num_decoder_tokens,max_encoder_seq_length,max_decoder_seq_length\n"
      ],
      "metadata": {
        "id": "qmpsxfA5XacG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Data"
      ],
      "metadata": {
        "id": "8AjqTAoSkZ3_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hXYqSWZPz9Xf"
      },
      "outputs": [],
      "source": [
        "train_source=open('/content/drive/MyDrive/Colab Notebooks/train-source.txt','r',encoding = \"UTF-8\").readlines()\n",
        "test_source=open('/content/drive/MyDrive/Colab Notebooks/train-target.txt','r',encoding = \"UTF-8\").readlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_sentences=conv_source_sentences(train_source)\n",
        "target_sentences=conv_target_sentences(test_source)\n",
        "print(\"Source Sentences: \\n\",source_sentences[:1])\n",
        "print(\"Target Sentences: \\n\",target_sentences[:1])"
      ],
      "metadata": {
        "id": "e2AFWJhay7M3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e8c419-2a7c-41db-a156-49f4753d9c21"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentences: \n",
            " ['Cinnte go leór , thiocfadh dóbhtha bás a fhagháil ar imeall an phuill udaí .']\n",
            "Target Sentences: \n",
            " ['\\tCinnte go leor , thiocfadh dóibh bás a fháil ar imeall an phoill úd .\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_characters,target_characters=inp_out_characters(source_sentences,target_sentences)\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
      ],
      "metadata": {
        "id": "G5TSZk0IaMoF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens,num_decoder_tokens,max_encoder_seq_length,max_decoder_seq_length=data_stats(input_characters,target_characters,source_sentences,target_sentences)\n",
        "print(\"Number of samples:\", len(source_sentences))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqJ2iMHwUy_3",
        "outputId": "76946cd4-555a-4780-9a00-224d28e6b5c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 45171\n",
            "Number of unique input tokens: 107\n",
            "Number of unique output tokens: 96\n",
            "Max sequence length for inputs: 1190\n",
            "Max sequence length for outputs: 1115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Validation Split"
      ],
      "metadata": {
        "id": "XZkdY5vJkjJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(source_sentences, target_sentences, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "iyCa2ICgF8rF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 128) :\n",
        "  #' Generate a batch of data \n",
        "  while True:\n",
        "    for j in range(0, len(X), batch_size):\n",
        "      encoder_input_data = np.zeros((batch_size, max_encoder_seq_length), dtype='float32')\n",
        "      decoder_input_data = np.zeros((batch_size, max_decoder_seq_length) , dtype= 'float32')\n",
        "      decoder_target_data = np.zeros((batch_size, max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "      for i,(input_text, target_text) in enumerate(zip (X[j:j+batch_size], y[j:j+batch_size])):\n",
        "        #print(input_text)\n",
        "        for t, word in enumerate(input_text):\n",
        "          #print(word)\n",
        "          encoder_input_data[i, t] = input_token_index[word] # encoder input seg\n",
        "        for t, word in enumerate(target_text):\n",
        "          if t<len(target_text)-1:\n",
        "            decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
        "          if t>0:\n",
        "            # decoder target sequence (one hot encoded)\n",
        "            # does not include the sTART token\n",
        "            # Offset by one timestep\n",
        "            decoder_target_data[i, t - 1, target_token_index [word]] = 1.\n",
        "      yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "metadata": {
        "id": "Vrb8DdchXaiX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Architecture"
      ],
      "metadata": {
        "id": "vhXr9W0Zxckz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input sequence and process it.\n",
        "latent_dim=256\n",
        "encoder_inputs = keras.Input(shape=(None,))\n",
        "enc_emb=Embedding(num_encoder_tokens,latent_dim,mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n"
      ],
      "metadata": {
        "id": "kedzpB7yGnVy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder Architecture"
      ],
      "metadata": {
        "id": "pEdsiloO6o9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None,))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "dec_emb_layer=Embedding(num_decoder_tokens,latent_dim,mask_zero=True)\n",
        "dec_emb=dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "eic5Y57R6qt_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 400\n",
        "epochs = 20\n",
        "#print(val_samples//batch_size)"
      ],
      "metadata": {
        "id": "foFoWZHQ6-0h"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator function to iteratively pull data and fit the model"
      ],
      "metadata": {
        "id": "SQegxhCRkq08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH3pae6o6pK7",
        "outputId": "38275cc6-809c-431f-a09b-44acd59d09e6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "101/101 [==============================] - 612s 6s/step - loss: 0.2045 - accuracy: 0.2794 - val_loss: 0.1738 - val_accuracy: 0.3565\n",
            "Epoch 2/20\n",
            "101/101 [==============================] - 594s 6s/step - loss: 0.1621 - accuracy: 0.3819 - val_loss: 0.1540 - val_accuracy: 0.4091\n",
            "Epoch 3/20\n",
            "101/101 [==============================] - 590s 6s/step - loss: 0.1491 - accuracy: 0.4239 - val_loss: 0.1439 - val_accuracy: 0.4453\n",
            "Epoch 4/20\n",
            "101/101 [==============================] - 589s 6s/step - loss: 0.1388 - accuracy: 0.4584 - val_loss: 0.1354 - val_accuracy: 0.4732\n",
            "Epoch 5/20\n",
            "101/101 [==============================] - 589s 6s/step - loss: 0.1304 - accuracy: 0.4907 - val_loss: 0.1272 - val_accuracy: 0.5041\n",
            "Epoch 6/20\n",
            "101/101 [==============================] - 586s 6s/step - loss: 0.1235 - accuracy: 0.5192 - val_loss: 0.1213 - val_accuracy: 0.5287\n",
            "Epoch 7/20\n",
            "101/101 [==============================] - 585s 6s/step - loss: 0.1179 - accuracy: 0.5416 - val_loss: 0.1154 - val_accuracy: 0.5530\n",
            "Epoch 8/20\n",
            "101/101 [==============================] - 586s 6s/step - loss: 0.1133 - accuracy: 0.5604 - val_loss: 0.1113 - val_accuracy: 0.5674\n",
            "Epoch 9/20\n",
            "101/101 [==============================] - 593s 6s/step - loss: 0.1095 - accuracy: 0.5758 - val_loss: 0.1084 - val_accuracy: 0.5791\n",
            "Epoch 10/20\n",
            "101/101 [==============================] - 585s 6s/step - loss: 0.1059 - accuracy: 0.5894 - val_loss: 0.1048 - val_accuracy: 0.5945\n",
            "Epoch 11/20\n",
            "101/101 [==============================] - 584s 6s/step - loss: 0.1030 - accuracy: 0.6008 - val_loss: 0.1023 - val_accuracy: 0.6045\n",
            "Epoch 12/20\n",
            "101/101 [==============================] - 581s 6s/step - loss: 0.1005 - accuracy: 0.6107 - val_loss: 0.0999 - val_accuracy: 0.6140\n",
            "Epoch 13/20\n",
            "101/101 [==============================] - 581s 6s/step - loss: 0.0984 - accuracy: 0.6191 - val_loss: 0.0982 - val_accuracy: 0.6203\n",
            "Epoch 14/20\n",
            "101/101 [==============================] - 583s 6s/step - loss: 0.0965 - accuracy: 0.6264 - val_loss: 0.0966 - val_accuracy: 0.6260\n",
            "Epoch 15/20\n",
            "101/101 [==============================] - 581s 6s/step - loss: 0.0947 - accuracy: 0.6328 - val_loss: 0.0950 - val_accuracy: 0.6330\n",
            "Epoch 16/20\n",
            "101/101 [==============================] - 580s 6s/step - loss: 0.0933 - accuracy: 0.6384 - val_loss: 0.0940 - val_accuracy: 0.6356\n",
            "Epoch 17/20\n",
            "101/101 [==============================] - 577s 6s/step - loss: 0.0918 - accuracy: 0.6434 - val_loss: 0.0925 - val_accuracy: 0.6412\n",
            "Epoch 18/20\n",
            "101/101 [==============================] - 577s 6s/step - loss: 0.0905 - accuracy: 0.6484 - val_loss: 0.0911 - val_accuracy: 0.6476\n",
            "Epoch 19/20\n",
            "101/101 [==============================] - 580s 6s/step - loss: 0.0893 - accuracy: 0.6531 - val_loss: 0.0897 - val_accuracy: 0.6529\n",
            "Epoch 20/20\n",
            "101/101 [==============================] - 576s 6s/step - loss: 0.0882 - accuracy: 0.6571 - val_loss: 0.0888 - val_accuracy: 0.6561\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1dca7f8050>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('/content/drive/MyDrive/Colab Notebooks/machine_translation_model.h5')"
      ],
      "metadata": {
        "id": "A8bI_GOUs5k4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/drive/MyDrive/Colab Notebooks/machine_translation_model.h5')"
      ],
      "metadata": {
        "id": "tC4vhhOHNbjy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "#decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "dec_emb2=dec_emb_layer(decoder_inputs)\n",
        "#decoder_lstm = model.layers[3]\n",
        "\n",
        "decoder_outputs2, state_h_dec, state_c_dec = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h_dec, state_c_dec]\n",
        "#decoder_dense = model.layers[4]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "zKOVGiWYbIh9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "metadata": {
        "id": "ffeuTSYObZ8K"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode Sequence Function"
      ],
      "metadata": {
        "id": "IwCr_wNTkzsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    #print(\"state value predictions: \",states_value)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0]= target_token_index['\\t']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "            print(\"Inside break cond\")\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] =sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "GTCHi8z6HBtv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen=generate_batch(X_train,y_train,batch_size=1)\n",
        "k=-1"
      ],
      "metadata": {
        "id": "0cFpSdCcjULp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XFhpG-RG-Krr",
        "outputId": "9fa36e34-8532-4234-a52a-9427e4e2b328"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cé go rabh siad ag bogadaigh go scaolmhar níor theich siad go rabh mé fá leath-duisín slat dóbhtha .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K0TLQW2w-mU9",
        "outputId": "d993623c-3666-4ea2-abc0-51ea8366cdde"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\tCé go raibh siad ag bogadaigh go scaollmhar níor theith siad go raibh mé fá leathdhoisín slat dóibh .\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k+=1\n",
        "print(k)\n",
        "(input_seq,actual_output),_=next(train_gen)\n",
        "decoded_sentence=decode_sequence(input_seq)\n",
        "print('Input Source sentence:', X_train[k:k+1])\n",
        "print('Actual Target Translation:', y_train[k:k+1])\n",
        "print('Predicted Target Translation:', decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoaNmsr1jjWa",
        "outputId": "abd4b15c-ee8c-4f95-8ea1-cc0b34db311f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "1/1 [==============================] - 0s 250ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Inside break cond\n",
            "Input Source sentence: ['Ag Droichead an Sceárdáin sceinn sí trasna an bhealaigh mhóir comh gasta le cairr-fhiadh .']\n",
            "Actual Target Translation: ['\\tAg Droichead an Scairdeáin scinn sí trasna an bhealaigh mhóir chomh gasta le carria .\\n']\n",
            "Predicted Target Translation: Is iomaí an t-am sin agus an t-am sin agus an t-am sin agus an t-am sin agus an t-am .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZXv09LROT28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}